import os
import argparse
import torch
from diffusers import AutoencoderKL, UNet2DModel, DDPMScheduler
from torchvision.utils import save_image
from torchvision.transforms import Grayscale
from tqdm import tqdm

# SD VAE í‘œì¤€ Scaling Factor
SD_SCALING_FACTOR = 0.18215

def generate_masks(args):
    device = torch.device(f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu")
    os.makedirs(args.output_dir, exist_ok=True)

    print(f"ğŸš€ SD-VAE LDM ì¶”ë¡  ì‹œì‘")
    print(f" - UNet ê²½ë¡œ: {args.unet_path}")
    print(f" - Scaling Factor: {SD_SCALING_FACTOR} (Fixed)")

    # 1. ëª¨ë¸ ë¡œë“œ
    # 1-1. Pretrained VAE
    vae = AutoencoderKL.from_pretrained(args.model_id, subfolder="vae").to(device)
    
    # 1-2. í•™ìŠµëœ UNet
    unet = UNet2DModel.from_pretrained(args.unet_path).to(device)
    
    # 1-3. ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = DDPMScheduler(num_train_timesteps=1000)

    unet.eval()
    vae.eval()

    # 2. ìƒì„± ë£¨í”„
    num_batches = (args.num_samples + args.batch_size - 1) // args.batch_size
    
    for i in range(num_batches):
        curr_batch_size = min(args.batch_size, args.num_samples - i * args.batch_size)
        
        # A. ëœë¤ ë…¸ì´ì¦ˆ (SD VAEëŠ” Latent Channelì´ 4)
        latents = torch.randn(
            (curr_batch_size, 4, args.resolution // 8, args.resolution // 8),
            device=device
        )

        # B. Denoising
        for t in tqdm(scheduler.timesteps, desc=f"Batch {i+1}/{num_batches}"):
            with torch.no_grad():
                noise_pred = unet(latents, t).sample
                latents = scheduler.step(noise_pred, t, latents).prev_sample

        # C. Decoding (Scale Factor ì ìš©)
        latents = latents / SD_SCALING_FACTOR
        with torch.no_grad():
            images = vae.decode(latents).sample

        # D. í›„ì²˜ë¦¬ ë° ì €ì¥
        images = (images / 2 + 0.5).clamp(0, 1)
        
        # SD VAEëŠ” ì¶œë ¥ì´ RGB(3ì±„ë„)ì´ë¯€ë¡œ, ë§ˆìŠ¤í¬ ìš©ë„ë¼ë©´ í‘ë°±(1ì±„ë„)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ”ê²Œ ì¢‹ìŒ
        if args.save_grayscale:
            # R,G,B í‰ê· ì„ ë‚´ì„œ 1ì±„ë„ë¡œ ë§Œë“¦
            images = images.mean(dim=1, keepdim=True)

        for j, img in enumerate(images):
            idx = i * args.batch_size + j
            save_path = os.path.join(args.output_dir, f"generated_{idx:04d}.png")
            save_image(img, save_path)

    print(f"ğŸ‰ ìƒì„± ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {args.output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--unet_path", type=str, required=True, help="í•™ìŠµëœ UNet í´ë” (ì˜ˆ: ldm_sd_result/best_unet)")
    parser.add_argument("--model_id", type=str, default="CompVis/stable-diffusion-v1-4", help="VAE ëª¨ë¸ ID")
    parser.add_argument("--output_dir", type=str, default="generated_samples_sd")
    parser.add_argument("--num_samples", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--resolution", type=int, default=512)
    parser.add_argument("--save_grayscale", action='store_true', help="ê²°ê³¼ë¥¼ í‘ë°±ìœ¼ë¡œ ì €ì¥í•˜ë ¤ë©´ ì‚¬ìš©")
    parser.add_argument("--gpu", type=int, default=0)
    
    args = parser.parse_args()
    generate_masks(args)